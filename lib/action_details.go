// **********************************************************************************************100
/*
Ollama Query - A simple command-line tool to interact with the Ollama server API.
Code to issue /api/show requests and display model details.

Created by Thomas.Cherry.gmail.com
*/

package lib

import (
	"bytes"
	"fmt"
	"io"
	"net/http"
	"strings"
)

// ... existing code ...

type ModelName struct {
	Model string `json:"model"`
}

type Modelfile struct {
	Details struct {
		Family            string   `json:"family"`
		Families          []string `json:"families"`
		Format            string   `json:"format"`
		ParentModel       string   `json:"parent_model"`
		ParameterSize     string   `json:"parameter_size"`
		QuantizationLevel string   `json:"quantization_level"`
	} `json:"details"`

	ModelInfo struct {
		GGML struct {
			BosTokenID    int      `json:"bos_token_id"`
			EOSTokenID    int      `json:"eos_token_id"`
			Merges        []string `json:"merges"`
			Pre           string   `json:"pre"`
			TokenType     []string `json:"token_type"`
			Tokens        []string `json:"tokens"`
			Model         string   `json:"model"`
			TokenizerType string   `json:"tokenizer.ggml.token_type"` // fix json path here
		} `json:"tokenizer.ggml"`

		Llama struct {
			Architecture       string `json:"llama.attention.head_count_kv"`
			ContextLength      int    `json:"llama.context_length"`
			BlockCount         int    `json:"llama.block_count"`
			FeedForwardLength  int    `json:"llama.feed_forward_length"`
			AttentionHeadCount int    `json:"llama.attention.head_count"`
			RopeDimensionCount int    `json:"llama.rope.dimension_count"`
			VocabSize          int    `json:"llama.vocab_size"`
			EmbeddingLength    int    `json:"llama.embedding_length"`
			FreqBase           int    `json:"llama.rope.freq_base"`
		} `json:"llama"`

		General struct {
			FileType            int    `json:"general.file_type"`
			ParameterCount      uint64 `json:"general.parameter_count"`
			QuantizationVersion int    `json:"general.quantization_version"`
			Architecture        string `json:"general.architecture"`
		} `json:"general"`

		Capabilities []string `json:"capabilities"`
	} `json:"model_info"`
}

/*
	curl http://localhost:11434/api/show -d '{
	  "model": "llava"
	}'

returns:

	{
	  modelfile: '# Modelfile generated by "ollama show"\n# To build a new Modelfile based on this one, replace the FROM line with:\n# FROM llava:latest\n\nFROM /Users/matt/.ollama/models/blobs/sha256:200765e1283640ffbd013184bf496e261032fa75b99498a9613be4e94d63ad52\nTEMPLATE """{{ .System }}\nUSER: {{ .Prompt }}\nASSISTANT: """\nPARAMETER num_ctx 4096\nPARAMETER stop "\u003c/s\u003e"\nPARAMETER stop "USER:"\nPARAMETER stop "ASSISTANT:"',
	  parameters: 'num_keep                       24\nstop                           "<|start_header_id|>"\nstop                           "<|end_header_id|>"\nstop                           "<|eot_id|>"',
	  template: "{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ .Response }}<|eot_id|>",
	  details: {
	    parent_model: "",
	    format: "gguf",
	    family: "llama",
	    families: ["llama"],
	    parameter_size: "8.0B",
	    quantization_level: "Q4_0",
	  },
	  model_info: {
	    "general.architecture": "llama",
	    "general.file_type": 2,
	    "general.parameter_count": 8030261248,
	    "general.quantization_version": 2,
	    "llama.attention.head_count": 32,
	    "llama.attention.head_count_kv": 8,
	    "llama.attention.layer_norm_rms_epsilon": 0.00001,
	    "llama.block_count": 32,
	    "llama.context_length": 8192,
	    "llama.embedding_length": 4096,
	    "llama.feed_forward_length": 14336,
	    "llama.rope.dimension_count": 128,
	    "llama.rope.freq_base": 500000,
	    "llama.vocab_size": 128256,
	    "tokenizer.ggml.bos_token_id": 128000,
	    "tokenizer.ggml.eos_token_id": 128009,
	    "tokenizer.ggml.merges": [], // populates if `verbose=true`
	    "tokenizer.ggml.model": "gpt2",
	    "tokenizer.ggml.pre": "llama-bpe",
	    "tokenizer.ggml.token_type": [], // populates if `verbose=true`
	    "tokenizer.ggml.tokens": [], // populates if `verbose=true`
	  },
	  capabilities: ["completion", "vision"],
	}
*/
func ShowModelDetails(context AppContext, params ...string) (map[string]string, error) {
	if len(params) < 1 {
		return nil, fmt.Errorf("no model name provided")
	}

	nameOfModel := params[0]
	modelName := ModelName{Model: nameOfModel}
	jsonBytes, err := JsonFromStruct(modelName)
	if err != nil {
		return nil, err
	}

	/*resp, err := http.Post(context.HostName+"/api/show",
	"application/json",
	io.NopCloser(bytes.NewReader(jsonBytes)))*/

	resp, err := http.Post(context.HostName+"/api/show", "application/json", bytes.NewReader(jsonBytes))
	if err != nil {
		resp.Body.Close()
		return nil, err
	}
	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, err
	}

	modelDetails, err := StructFromJson[Modelfile](body)
	if err != nil {
		return nil, err
	}

	fmt.Fprintln(context.Output, strings.Repeat("*", 80))
	fmt.Fprintf(context.Output, "Model Details for %s:\n", nameOfModel)
	fmt.Fprintf(context.Output, "Parent Model: %s\n", modelDetails.Details.ParentModel)
	fmt.Fprintf(context.Output, "Format: %s\n", modelDetails.Details.Format)
	fmt.Fprintf(context.Output, "Family: %s\n", modelDetails.Details.Family)
	fmt.Fprintf(context.Output, "Parameter Size: %s\n", modelDetails.Details.ParameterSize)
	fmt.Fprintf(context.Output, "Quantization Level: %s\n", modelDetails.Details.QuantizationLevel)
	return nil, nil
}
